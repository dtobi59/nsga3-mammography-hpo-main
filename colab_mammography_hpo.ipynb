{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/dtobi59/nsga3-mammography-hpo-main/blob/master/colab_mammography_hpo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NSGA-III Hyperparameter Optimization for Mammography Classification\n",
    "\n",
    "Multi-objective hyperparameter optimization for deep learning models in breast cancer detection.\n",
    "\n",
    "**Objectives:**\n",
    "- Maximize: Sensitivity, Specificity, AUC\n",
    "- Minimize: Model Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Clone Repository & Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository\n",
    "!git clone https://github.com/dtobi59/nsga3-mammography-hpo-main.git\n",
    "%cd nsga3-mammography-hpo-main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Download Kaggle VinDr-Mammo PNG Dataset (RECOMMENDED)\n",
    "\n",
    "**This dataset is 5-10x faster than DICOM!**\n",
    "\n",
    "Follow these 4 steps to download the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: Install Kaggle CLI\n",
    "!pip install -q kaggle\n",
    "\n",
    "print(\"Kaggle CLI installed!\")\n",
    "print(\"\\nNext: Upload your kaggle.json file in the cell below.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: Upload your kaggle.json file\n",
    "# Get it from: https://www.kaggle.com/settings -> Create New API Token\n",
    "\n",
    "from google.colab import files\n",
    "\n",
    "print(\"Please upload your kaggle.json file:\")\n",
    "print(\"(Go to https://www.kaggle.com/settings and click 'Create New API Token')\")\n",
    "print()\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Setup Kaggle credentials\n",
    "!mkdir -p ~/.kaggle\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n",
    "\n",
    "print(\"\\nKaggle credentials configured!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3: Download the dataset\n",
    "print(\"Downloading Kaggle VinDr-Mammo PNG dataset...\")\n",
    "print(\"Dataset size: ~8.25 GB - this will take a few minutes\\n\")\n",
    "\n",
    "!kaggle datasets download -d shantanughosh/vindr-mammogram-dataset-dicom-to-png\n",
    "\n",
    "print(\"\\nDownload complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 4: Extract to Google Drive\n",
    "print(\"Extracting dataset to Google Drive...\")\n",
    "print(\"This will be saved to: /content/drive/MyDrive/kaggle_vindr_data\\n\")\n",
    "\n",
    "!unzip -q vindr-mammogram-dataset-dicom-to-png.zip -d /content/drive/MyDrive/kaggle_vindr_data\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DATASET READY!\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nLocation: /content/drive/MyDrive/kaggle_vindr_data\")\n",
    "print(\"\\nThe dataset is now ready to use!\")\n",
    "print(\"Continue to the next section to configure and load it.\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Configure Dataset Path\n",
    "\n",
    "**After downloading the dataset above**, the paths below should work with the default settings.\n",
    "\n",
    "If you're using a different dataset (VinDr DICOM or INbreast), uncomment the appropriate option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure your dataset\n",
    "\n",
    "# OPTION A: Kaggle VinDr-Mammo PNG (Recommended - 5-10x faster!)\n",
    "# Use this if you downloaded the dataset using cells above\n",
    "DATASET_NAME = \"kaggle_vindr_png\"\n",
    "DATA_ROOT = \"/content/drive/MyDrive/kaggle_vindr_data\"\n",
    "\n",
    "# OPTION B: Original VinDr-Mammo DICOM (slower)\n",
    "# Use this if you already have VinDr-Mammo DICOM dataset\n",
    "# DATASET_NAME = \"vindr\"\n",
    "# DATA_ROOT = \"/content/drive/MyDrive/vindr-mammo\"\n",
    "\n",
    "# OPTION C: INbreast DICOM (slower)\n",
    "# DATASET_NAME = \"inbreast\"\n",
    "# DATA_ROOT = \"/content/drive/MyDrive/inbreast\"\n",
    "\n",
    "# Output directory (where results will be saved)\n",
    "OUTPUT_DIR = \"/content/drive/MyDrive/nsga3_outputs\"\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"DATASET CONFIGURATION\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Dataset: {DATASET_NAME}\")\n",
    "print(f\"Data root: {DATA_ROOT}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test Your Setup (Optional but Recommended)\n",
    "\n",
    "Run this quick test to verify everything is working before starting the full workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run setup verification test\n",
    "!python test_colab_setup.py --data_root {DATA_ROOT}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import prepare_dataset\n",
    "\n",
    "print(f\"Loading {DATASET_NAME} dataset from {DATA_ROOT}...\")\n",
    "\n",
    "train_paths, train_labels, val_paths, val_labels = prepare_dataset(\n",
    "    dataset_name=DATASET_NAME,\n",
    "    data_root=DATA_ROOT\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset loaded successfully!\")\n",
    "print(f\"Train samples: {len(train_paths)}\")\n",
    "print(f\"Validation samples: {len(val_paths)}\")\n",
    "print(f\"Train labels distribution: {sum(train_labels)} malignant, {len(train_labels) - sum(train_labels)} benign\")\n",
    "print(f\"Val labels distribution: {sum(val_labels)} malignant, {len(val_labels) - sum(val_labels)} benign\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualize Sample Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from dataset import load_dicom_image\n",
    "from PIL import Image\n",
    "\n",
    "def visualize_samples(image_paths, labels, dataset_name, n_samples=6):\n",
    "    \"\"\"\n",
    "    Visualize sample mammography images.\n",
    "    \"\"\"\n",
    "    # Select random samples\n",
    "    n_samples = min(n_samples, len(image_paths))\n",
    "    indices = np.random.choice(len(image_paths), n_samples, replace=False)\n",
    "\n",
    "    # Create subplot grid\n",
    "    n_cols = 3\n",
    "    n_rows = (n_samples + n_cols - 1) // n_cols\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5*n_rows))\n",
    "\n",
    "    if n_rows == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "\n",
    "    fig.suptitle(f'{dataset_name} Dataset Samples', fontsize=16, fontweight='bold')\n",
    "\n",
    "    for idx, ax_idx in enumerate(range(n_samples)):\n",
    "        row = ax_idx // n_cols\n",
    "        col = ax_idx % n_cols\n",
    "        ax = axes[row, col]\n",
    "\n",
    "        sample_idx = indices[idx]\n",
    "        img_path = image_paths[sample_idx]\n",
    "        label = labels[sample_idx]\n",
    "\n",
    "        try:\n",
    "            # Load image (supports both DICOM and PNG)\n",
    "            if img_path.lower().endswith(('.dcm', '.dicom')):\n",
    "                image = load_dicom_image(img_path, apply_clahe=True)\n",
    "            else:\n",
    "                # PNG/JPG image\n",
    "                img = Image.open(img_path)\n",
    "                if img.mode != 'RGB':\n",
    "                    img = img.convert('RGB')\n",
    "                image = np.array(img)\n",
    "\n",
    "            # Display\n",
    "            ax.imshow(image, cmap='gray')\n",
    "            label_text = 'MALIGNANT' if label == 1 else 'BENIGN'\n",
    "            label_color = 'red' if label == 1 else 'green'\n",
    "            ax.set_title(f'{label_text}', fontsize=12, fontweight='bold', color=label_color)\n",
    "            ax.axis('off')\n",
    "\n",
    "            # Add filename as subtitle\n",
    "            filename = os.path.basename(img_path)\n",
    "            ax.text(0.5, -0.05, filename[:30] + '...' if len(filename) > 30 else filename,\n",
    "                   ha='center', va='top', transform=ax.transAxes,\n",
    "                   fontsize=8, style='italic')\n",
    "\n",
    "        except Exception as e:\n",
    "            ax.text(0.5, 0.5, f'Error loading image:\\n{str(e)[:50]}',\n",
    "                   ha='center', va='center', transform=ax.transAxes)\n",
    "            ax.axis('off')\n",
    "\n",
    "    # Hide extra subplots\n",
    "    for idx in range(n_samples, n_rows * n_cols):\n",
    "        row = idx // n_cols\n",
    "        col = idx % n_cols\n",
    "        axes[row, col].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize training samples\n",
    "print(\"=\" * 70)\n",
    "print(\"TRAINING SET SAMPLES\")\n",
    "print(\"=\" * 70)\n",
    "visualize_samples(train_paths, train_labels, 'Training', n_samples=6)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"VALIDATION SET SAMPLES\")\n",
    "print(\"=\" * 70)\n",
    "visualize_samples(val_paths, val_labels, 'Validation', n_samples=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Configure Optimization\n",
    "\n",
    "Adjust these parameters based on your computational budget:\n",
    "- **pop_size**: Population size (more = better exploration, longer runtime)\n",
    "- **n_generations**: Number of generations (more = better convergence)\n",
    "- **epochs**: Training epochs per evaluation (reduce for faster testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import ExperimentConfig\n",
    "from dataclasses import asdict\n",
    "\n",
    "# Load default config\n",
    "config = ExperimentConfig()\n",
    "\n",
    "# Customize NSGA-III parameters\n",
    "config.nsga3.pop_size = 20        # Recommended: 20-50 for quick runs, 100+ for thorough search\n",
    "config.nsga3.n_generations = 10   # Recommended: 5-10 for testing, 20-50 for production\n",
    "\n",
    "# Training epochs per evaluation (reduce for faster testing)\n",
    "TRAINING_EPOCHS = 5  # Recommended: 5 for testing, 10-20 for production\n",
    "\n",
    "print(f\"Optimization Configuration:\")\n",
    "print(f\"  Population size: {config.nsga3.pop_size}\")\n",
    "print(f\"  Generations: {config.nsga3.n_generations}\")\n",
    "print(f\"  Epochs per evaluation: {TRAINING_EPOCHS}\")\n",
    "print(f\"  Total evaluations: ~{config.nsga3.pop_size * config.nsga3.n_generations}\")\n",
    "print(f\"\\nSearching hyperparameters:\")\n",
    "\n",
    "# Convert dataclass to dictionary for iteration\n",
    "hp_space_dict = asdict(config.hyperparameter_space)\n",
    "for param, values in hp_space_dict.items():\n",
    "    print(f\"  {param}: {values}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Create Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training import full_evaluation\n",
    "\n",
    "def make_eval_fn(tp, tl, vp, vl, epochs=5):\n",
    "    \"\"\"\n",
    "    Creates an evaluation function for the optimizer.\n",
    "    \"\"\"\n",
    "    def eval_fn(hp_config):\n",
    "        hp_config = hp_config.copy()\n",
    "        hp_config['epochs'] = epochs\n",
    "        return full_evaluation(\n",
    "            hp_config,\n",
    "            tp, tl, vp, vl,\n",
    "            device='cuda',  # Use GPU\n",
    "            verbose=True\n",
    "        )\n",
    "    return eval_fn\n",
    "\n",
    "eval_function = make_eval_fn(\n",
    "    train_paths, train_labels,\n",
    "    val_paths, val_labels,\n",
    "    epochs=TRAINING_EPOCHS\n",
    ")\n",
    "\n",
    "print(\"Evaluation function created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Run Optimization\n",
    "\n",
    "This will take some time depending on your configuration.\n",
    "\n",
    "**Estimated runtime:**\n",
    "- Small (pop=20, gen=5, epochs=5): ~30-60 minutes\n",
    "- Medium (pop=50, gen=10, epochs=10): ~2-4 hours\n",
    "- Large (pop=100, gen=20, epochs=20): ~8-12 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimization import run_optimization\n",
    "import time\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "print(f\"Starting optimization with seed={RANDOM_SEED}...\\n\")\n",
    "start_time = time.time()\n",
    "\n",
    "results = run_optimization(\n",
    "    hp_space=config.hyperparameter_space,\n",
    "    nsga_config=config.nsga3,\n",
    "    eval_function=eval_function,\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    seed=RANDOM_SEED\n",
    ")\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Optimization completed in {elapsed_time/3600:.2f} hours\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Pareto front solutions\n",
    "print(f\"\\nFound {len(results['pareto_configs'])} Pareto-optimal solutions\\n\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "for i, (cfg, obj) in enumerate(zip(results['pareto_configs'], results['pareto_F'])):\n",
    "    print(f\"\\nSolution {i+1}:\")\n",
    "    print(\"-\" * 100)\n",
    "\n",
    "    # Architecture details\n",
    "    print(f\"  Architecture:\")\n",
    "    print(f\"    Backbone: {cfg['backbone']}\")\n",
    "    print(f\"    Unfreeze: {cfg['unfreeze_strategy']}\")\n",
    "    print(f\"    Dropout: {cfg['dropout_rate']:.2f}\")\n",
    "    print(f\"    FC Hidden: {cfg['fc_hidden_size']}\")\n",
    "\n",
    "    # Training details\n",
    "    print(f\"  Training:\")\n",
    "    print(f\"    Optimizer: {cfg['optimizer']}\")\n",
    "    print(f\"    Learning Rate: {cfg['learning_rate']:.6f}\")\n",
    "    print(f\"    Batch Size: {cfg['batch_size']}\")\n",
    "    print(f\"    Loss: {cfg['loss_function']}\")\n",
    "\n",
    "    # Performance metrics\n",
    "    print(f\"  Performance:\")\n",
    "    print(f\"    Sensitivity: {obj[0]:.4f}\")\n",
    "    print(f\"    Specificity: {obj[1]:.4f}\")\n",
    "    print(f\"    AUC: {obj[2]:.4f}\")\n",
    "    print(f\"    Model Size: {obj[3]:.2f}M parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Visualize Pareto Front"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Extract objectives\n",
    "pareto_F = np.array(results['pareto_F'])\n",
    "\n",
    "# Create visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('Pareto Front - Trade-off Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Sensitivity vs Specificity\n",
    "axes[0, 0].scatter(pareto_F[:, 0], pareto_F[:, 1], c='blue', s=100, alpha=0.6)\n",
    "axes[0, 0].set_xlabel('Sensitivity', fontsize=12)\n",
    "axes[0, 0].set_ylabel('Specificity', fontsize=12)\n",
    "axes[0, 0].set_title('Sensitivity vs Specificity')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# AUC vs Model Size\n",
    "axes[0, 1].scatter(pareto_F[:, 2], pareto_F[:, 3], c='green', s=100, alpha=0.6)\n",
    "axes[0, 1].set_xlabel('AUC', fontsize=12)\n",
    "axes[0, 1].set_ylabel('Model Size (M params)', fontsize=12)\n",
    "axes[0, 1].set_title('AUC vs Model Size')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Sensitivity vs Model Size\n",
    "axes[1, 0].scatter(pareto_F[:, 0], pareto_F[:, 3], c='purple', s=100, alpha=0.6)\n",
    "axes[1, 0].set_xlabel('Sensitivity', fontsize=12)\n",
    "axes[1, 0].set_ylabel('Model Size (M params)', fontsize=12)\n",
    "axes[1, 0].set_title('Sensitivity vs Model Size')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Overall metrics distribution\n",
    "metrics = ['Sensitivity', 'Specificity', 'AUC', 'Size (M)']\n",
    "avg_values = pareto_F.mean(axis=0)\n",
    "axes[1, 1].bar(metrics, avg_values, color=['blue', 'green', 'red', 'purple'], alpha=0.6)\n",
    "axes[1, 1].set_ylabel('Average Value', fontsize=12)\n",
    "axes[1, 1].set_title('Average Objective Values')\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{OUTPUT_DIR}/pareto_front_visualization.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nVisualization saved to {OUTPUT_DIR}/pareto_front_visualization.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Save Best Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Find solution with best AUC\n",
    "best_auc_idx = np.argmax(pareto_F[:, 2])\n",
    "best_auc_config = results['pareto_configs'][best_auc_idx]\n",
    "best_auc_objectives = pareto_F[best_auc_idx]\n",
    "\n",
    "print(f\"Best AUC Configuration:\")\n",
    "print(f\"  AUC: {best_auc_objectives[2]:.4f}\")\n",
    "print(f\"  Sensitivity: {best_auc_objectives[0]:.4f}\")\n",
    "print(f\"  Specificity: {best_auc_objectives[1]:.4f}\")\n",
    "print(f\"\\nConfiguration: {json.dumps(best_auc_config, indent=2)}\")\n",
    "\n",
    "# Save to file\n",
    "with open(f\"{OUTPUT_DIR}/best_auc_config.json\", 'w') as f:\n",
    "    json.dump({\n",
    "        'config': best_auc_config,\n",
    "        'objectives': {\n",
    "            'sensitivity': float(best_auc_objectives[0]),\n",
    "            'specificity': float(best_auc_objectives[1]),\n",
    "            'auc': float(best_auc_objectives[2]),\n",
    "            'model_size_M': float(best_auc_objectives[3])\n",
    "        }\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(f\"\\nBest configuration saved to {OUTPUT_DIR}/best_auc_config.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Results Summary\n",
    "\n",
    "All results have been saved to your Google Drive:\n",
    "- Pareto-optimal configurations\n",
    "- Optimization history\n",
    "- Visualizations\n",
    "- Best configuration JSON\n",
    "\n",
    "You can now:\n",
    "1. Download the results from Google Drive\n",
    "2. Use the best configuration to train a final model\n",
    "3. Deploy the model for inference\n",
    "4. Run additional experiments with different parameters"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
