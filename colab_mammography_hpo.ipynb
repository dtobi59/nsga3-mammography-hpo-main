{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NSGA-III Hyperparameter Optimization for Mammography Classification\n",
    "\n",
    "Multi-objective hyperparameter optimization for deep learning models in breast cancer detection.\n",
    "\n",
    "**Objectives:**\n",
    "- Maximize: Sensitivity, Specificity, AUC\n",
    "- Minimize: Model Size, Inference Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Clone Repository & Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Clone the repository\n!git clone https://github.com/dtobi59/-nsga3-mammography.git\n%cd -nsga3-mammography"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Configure Dataset Path\n",
    "\n",
    "Update the path below to point to your dataset on Google Drive.\n",
    "\n",
    "**Supported datasets:**\n",
    "- `vindr` - VinDr-Mammo\n",
    "- `inbreast` - INbreast\n",
    "\n",
    "**Expected structure:**\n",
    "\n",
    "**VinDr-Mammo:**\n",
    "```\n",
    "vindr-mammo/\n",
    "├── images/\n",
    "│   └── {study_id}/\n",
    "│       └── {image_id}.dicom\n",
    "└── metadata/\n",
    "    └── breast-level_annotations.csv\n",
    "```\n",
    "\n",
    "**INbreast:**\n",
    "```\n",
    "inbreast/\n",
    "└── INbreast Release 1.0/\n",
    "    ├── AllDICOMs/\n",
    "    │   └── {id}_{hash}_MG_{L/R}_{CC/MLO}_ANON.dcm\n",
    "    └── INbreast.csv\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure your dataset\n",
    "DATASET_NAME = \"vindr\"  # or \"inbreast\"\n",
    "DATA_ROOT = \"/content/drive/MyDrive/vindr-mammo\"  # Update this path!\n",
    "OUTPUT_DIR = \"/content/drive/MyDrive/nsga3_outputs\"  # Where to save results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import prepare_dataset\n",
    "\n",
    "print(f\"Loading {DATASET_NAME} dataset from {DATA_ROOT}...\")\n",
    "\n",
    "train_paths, train_labels, val_paths, val_labels = prepare_dataset(\n",
    "    dataset_name=DATASET_NAME,\n",
    "    data_root=DATA_ROOT\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset loaded successfully!\")\n",
    "print(f\"Train samples: {len(train_paths)}\")\n",
    "print(f\"Validation samples: {len(val_paths)}\")\n",
    "print(f\"Train labels distribution: {sum(train_labels)} malignant, {len(train_labels) - sum(train_labels)} benign\")\n",
    "print(f\"Val labels distribution: {sum(val_labels)} malignant, {len(val_labels) - sum(val_labels)} benign\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Configure Optimization\n",
    "\n",
    "Adjust these parameters based on your computational budget:\n",
    "- **pop_size**: Population size (more = better exploration, longer runtime)\n",
    "- **n_generations**: Number of generations (more = better convergence)\n",
    "- **epochs**: Training epochs per evaluation (reduce for faster testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import ExperimentConfig\n",
    "\n",
    "# Load default config\n",
    "config = ExperimentConfig()\n",
    "\n",
    "# Customize NSGA-III parameters\n",
    "config.nsga3.pop_size = 20        # Recommended: 20-50 for quick runs, 100+ for thorough search\n",
    "config.nsga3.n_generations = 10   # Recommended: 5-10 for testing, 20-50 for production\n",
    "\n",
    "# Training epochs per evaluation (reduce for faster testing)\n",
    "TRAINING_EPOCHS = 5  # Recommended: 5 for testing, 10-20 for production\n",
    "\n",
    "print(f\"Optimization Configuration:\")\n",
    "print(f\"  Population size: {config.nsga3.pop_size}\")\n",
    "print(f\"  Generations: {config.nsga3.n_generations}\")\n",
    "print(f\"  Epochs per evaluation: {TRAINING_EPOCHS}\")\n",
    "print(f\"  Total evaluations: ~{config.nsga3.pop_size * config.nsga3.n_generations}\")\n",
    "print(f\"\\nSearching hyperparameters:\")\n",
    "for param, values in config.hyperparameter_space.items():\n",
    "    print(f\"  {param}: {values}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Create Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training import full_evaluation\n",
    "\n",
    "def make_eval_fn(tp, tl, vp, vl, epochs=5):\n",
    "    \"\"\"\n",
    "    Creates an evaluation function for the optimizer.\n",
    "    \n",
    "    Args:\n",
    "        tp: Training paths\n",
    "        tl: Training labels\n",
    "        vp: Validation paths\n",
    "        vl: Validation labels\n",
    "        epochs: Number of training epochs\n",
    "    \n",
    "    Returns:\n",
    "        Evaluation function that takes hyperparameter config and returns objectives\n",
    "    \"\"\"\n",
    "    def eval_fn(hp_config):\n",
    "        hp_config = hp_config.copy()\n",
    "        hp_config['epochs'] = epochs\n",
    "        return full_evaluation(\n",
    "            hp_config, \n",
    "            tp, tl, vp, vl, \n",
    "            device='cuda',  # Use GPU\n",
    "            verbose=True\n",
    "        )\n",
    "    return eval_fn\n",
    "\n",
    "eval_function = make_eval_fn(\n",
    "    train_paths, train_labels, \n",
    "    val_paths, val_labels,\n",
    "    epochs=TRAINING_EPOCHS\n",
    ")\n",
    "\n",
    "print(\"Evaluation function created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Run Optimization\n",
    "\n",
    "This will take some time depending on your configuration.\n",
    "\n",
    "**Estimated runtime:**\n",
    "- Small (pop=20, gen=5, epochs=5): ~30-60 minutes\n",
    "- Medium (pop=50, gen=10, epochs=10): ~2-4 hours\n",
    "- Large (pop=100, gen=20, epochs=20): ~8-12 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimization import run_optimization\n",
    "import time\n",
    "\n",
    "print(\"Starting optimization...\\n\")\n",
    "start_time = time.time()\n",
    "\n",
    "results = run_optimization(\n",
    "    hp_space=config.hyperparameter_space,\n",
    "    nsga_config=config.nsga3,\n",
    "    eval_function=eval_function,\n",
    "    output_dir=OUTPUT_DIR\n",
    ")\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Optimization completed in {elapsed_time/3600:.2f} hours\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Pareto front solutions\n",
    "print(f\"\\nFound {len(results['pareto_configs'])} Pareto-optimal solutions\\n\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "for i, (cfg, obj) in enumerate(zip(results['pareto_configs'], results['pareto_F'])):\n",
    "    print(f\"\\nSolution {i+1}:\")\n",
    "    print(\"-\" * 100)\n",
    "    \n",
    "    # Architecture details\n",
    "    print(f\"  Architecture:\")\n",
    "    print(f\"    Backbone: {cfg['backbone']}\")\n",
    "    print(f\"    Unfreeze: {cfg['unfreeze_strategy']}\")\n",
    "    print(f\"    Dropout: {cfg['dropout_rate']:.2f}\")\n",
    "    print(f\"    FC Hidden: {cfg['fc_hidden_size']}\")\n",
    "    \n",
    "    # Training details\n",
    "    print(f\"  Training:\")\n",
    "    print(f\"    Optimizer: {cfg['optimizer']}\")\n",
    "    print(f\"    Learning Rate: {cfg['learning_rate']:.6f}\")\n",
    "    print(f\"    Batch Size: {cfg['batch_size']}\")\n",
    "    print(f\"    Loss: {cfg['loss_function']}\")\n",
    "    if cfg['loss_function'] == 'focal':\n",
    "        print(f\"    Focal Gamma: {cfg.get('focal_gamma', 2.0):.2f}\")\n",
    "    \n",
    "    # Augmentation\n",
    "    print(f\"  Augmentation:\")\n",
    "    print(f\"    Horizontal Flip: {cfg['horizontal_flip']}\")\n",
    "    print(f\"    Rotation: {cfg['rotation_range']:.1f}°\")\n",
    "    print(f\"    Mixup: {cfg['use_mixup']}\")\n",
    "    \n",
    "    # Performance metrics\n",
    "    print(f\"  Performance:\")\n",
    "    print(f\"    Sensitivity: {obj[0]:.4f}\")\n",
    "    print(f\"    Specificity: {obj[1]:.4f}\")\n",
    "    print(f\"    AUC: {obj[2]:.4f}\")\n",
    "    print(f\"    Model Size: {obj[3]:.2f}M parameters\")\n",
    "    print(f\"    Inference Time: {obj[4]:.2f}ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Visualize Pareto Front (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Extract objectives\n",
    "pareto_F = np.array(results['pareto_F'])\n",
    "\n",
    "# Create visualizations\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "fig.suptitle('Pareto Front - Trade-off Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Sensitivity vs Specificity\n",
    "axes[0, 0].scatter(pareto_F[:, 0], pareto_F[:, 1], c='blue', s=100, alpha=0.6)\n",
    "axes[0, 0].set_xlabel('Sensitivity', fontsize=12)\n",
    "axes[0, 0].set_ylabel('Specificity', fontsize=12)\n",
    "axes[0, 0].set_title('Sensitivity vs Specificity')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# AUC vs Model Size\n",
    "axes[0, 1].scatter(pareto_F[:, 2], pareto_F[:, 3], c='green', s=100, alpha=0.6)\n",
    "axes[0, 1].set_xlabel('AUC', fontsize=12)\n",
    "axes[0, 1].set_ylabel('Model Size (M params)', fontsize=12)\n",
    "axes[0, 1].set_title('AUC vs Model Size')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# AUC vs Inference Time\n",
    "axes[0, 2].scatter(pareto_F[:, 2], pareto_F[:, 4], c='red', s=100, alpha=0.6)\n",
    "axes[0, 2].set_xlabel('AUC', fontsize=12)\n",
    "axes[0, 2].set_ylabel('Inference Time (ms)', fontsize=12)\n",
    "axes[0, 2].set_title('AUC vs Inference Time')\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# Sensitivity vs Model Size\n",
    "axes[1, 0].scatter(pareto_F[:, 0], pareto_F[:, 3], c='purple', s=100, alpha=0.6)\n",
    "axes[1, 0].set_xlabel('Sensitivity', fontsize=12)\n",
    "axes[1, 0].set_ylabel('Model Size (M params)', fontsize=12)\n",
    "axes[1, 0].set_title('Sensitivity vs Model Size')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Model Size vs Inference Time\n",
    "axes[1, 1].scatter(pareto_F[:, 3], pareto_F[:, 4], c='orange', s=100, alpha=0.6)\n",
    "axes[1, 1].set_xlabel('Model Size (M params)', fontsize=12)\n",
    "axes[1, 1].set_ylabel('Inference Time (ms)', fontsize=12)\n",
    "axes[1, 1].set_title('Model Size vs Inference Time')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Overall metrics distribution\n",
    "metrics = ['Sensitivity', 'Specificity', 'AUC', 'Size (M)', 'Time (ms)']\n",
    "avg_values = pareto_F.mean(axis=0)\n",
    "axes[1, 2].bar(metrics, avg_values, color=['blue', 'green', 'red', 'purple', 'orange'], alpha=0.6)\n",
    "axes[1, 2].set_ylabel('Average Value', fontsize=12)\n",
    "axes[1, 2].set_title('Average Objective Values')\n",
    "axes[1, 2].tick_params(axis='x', rotation=45)\n",
    "axes[1, 2].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{OUTPUT_DIR}/pareto_front_visualization.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nVisualization saved to {OUTPUT_DIR}/pareto_front_visualization.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save Best Configuration for Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Find solution with best AUC\n",
    "best_auc_idx = np.argmax(pareto_F[:, 2])\n",
    "best_auc_config = results['pareto_configs'][best_auc_idx]\n",
    "best_auc_objectives = pareto_F[best_auc_idx]\n",
    "\n",
    "print(f\"Best AUC Configuration:\")\n",
    "print(f\"  AUC: {best_auc_objectives[2]:.4f}\")\n",
    "print(f\"  Sensitivity: {best_auc_objectives[0]:.4f}\")\n",
    "print(f\"  Specificity: {best_auc_objectives[1]:.4f}\")\n",
    "print(f\"\\nConfiguration: {json.dumps(best_auc_config, indent=2)}\")\n",
    "\n",
    "# Save to file\n",
    "with open(f\"{OUTPUT_DIR}/best_auc_config.json\", 'w') as f:\n",
    "    json.dump({\n",
    "        'config': best_auc_config,\n",
    "        'objectives': {\n",
    "            'sensitivity': float(best_auc_objectives[0]),\n",
    "            'specificity': float(best_auc_objectives[1]),\n",
    "            'auc': float(best_auc_objectives[2]),\n",
    "            'model_size_M': float(best_auc_objectives[3]),\n",
    "            'inference_time_ms': float(best_auc_objectives[4])\n",
    "        }\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(f\"\\nBest configuration saved to {OUTPUT_DIR}/best_auc_config.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Select Configuration by Preference (Optional)\n",
    "\n",
    "Choose a configuration based on your priorities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find different optimal solutions\n",
    "best_sensitivity_idx = np.argmax(pareto_F[:, 0])\n",
    "best_specificity_idx = np.argmax(pareto_F[:, 1])\n",
    "smallest_model_idx = np.argmin(pareto_F[:, 3])\n",
    "fastest_inference_idx = np.argmin(pareto_F[:, 4])\n",
    "\n",
    "print(\"Different Optimization Preferences:\\n\")\n",
    "\n",
    "print(f\"1. Best Sensitivity: {pareto_F[best_sensitivity_idx, 0]:.4f}\")\n",
    "print(f\"   AUC: {pareto_F[best_sensitivity_idx, 2]:.4f}, Size: {pareto_F[best_sensitivity_idx, 3]:.2f}M\\n\")\n",
    "\n",
    "print(f\"2. Best Specificity: {pareto_F[best_specificity_idx, 1]:.4f}\")\n",
    "print(f\"   AUC: {pareto_F[best_specificity_idx, 2]:.4f}, Size: {pareto_F[best_specificity_idx, 3]:.2f}M\\n\")\n",
    "\n",
    "print(f\"3. Best AUC: {pareto_F[best_auc_idx, 2]:.4f}\")\n",
    "print(f\"   Sens: {pareto_F[best_auc_idx, 0]:.4f}, Spec: {pareto_F[best_auc_idx, 1]:.4f}\\n\")\n",
    "\n",
    "print(f\"4. Smallest Model: {pareto_F[smallest_model_idx, 3]:.2f}M\")\n",
    "print(f\"   AUC: {pareto_F[smallest_model_idx, 2]:.4f}, Time: {pareto_F[smallest_model_idx, 4]:.2f}ms\\n\")\n",
    "\n",
    "print(f\"5. Fastest Inference: {pareto_F[fastest_inference_idx, 4]:.2f}ms\")\n",
    "print(f\"   AUC: {pareto_F[fastest_inference_idx, 2]:.4f}, Size: {pareto_F[fastest_inference_idx, 3]:.2f}M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Results Summary\n",
    "\n",
    "All results have been saved to your Google Drive:\n",
    "- Pareto-optimal configurations\n",
    "- Optimization history\n",
    "- Visualizations\n",
    "- Best configuration JSON\n",
    "\n",
    "You can now:\n",
    "1. Download the results from Google Drive\n",
    "2. Use the best configuration to train a final model\n",
    "3. Deploy the model for inference\n",
    "4. Run additional experiments with different parameters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}